{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# Reproducibility- Now this part contributes to 3 things in our code:\n",
    "#1. It fixes all sorts of randomness including the CPU GPU numpy and Python\n",
    "#2. It forces convulsion algorigthms\n",
    "#3. It disables cuDNN benchmarks- we can say from this that it diables dynamic algorithm selection\n",
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "#This is very important for comparision between plain CNN and Residual CNN\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "id": "b9174e3878797d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-02T06:29:40.702734785Z",
     "start_time": "2026-03-02T06:29:40.650080390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Now this part of cell is for data loading:\n",
    "def get_dataloaders(batch_size=128):\n",
    "    #Doing all this will make sure that the generalization is improve dand overfitting is reduced\n",
    "    train_transform = transforms.Compose([\n",
    "        #the below statememnt addds padding to the image by 4 and add translation invariance\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        #Now we flip 50% of the images\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(\n",
    "            brightness=0.2,\n",
    "            contrast=0.2,\n",
    "            saturation=0.2,\n",
    "            hue=0.1\n",
    "        ),\n",
    "        #Now we convert this HWC image to CHW tensor\n",
    "        transforms.ToTensor(),\n",
    "        #We then apply channel wise normalization:\n",
    "        transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "    ])\n",
    "    #We now give it our real data set CIFAR10\n",
    "    full_train = torchvision.datasets.CIFAR10(\n",
    "        root='./data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    #We are going to be splitting the data again into training and testing and be following the 8:2 rule split\n",
    "    val_size = 5000\n",
    "    train_size = len(full_train) - val_size\n",
    "    train_dataset, val_dataset = random_split(full_train, [train_size, val_size])\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root='./data',\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=test_transform\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader, test_loader"
   ],
   "id": "cbb3bbeafbae8524",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-02T06:29:40.746671294Z",
     "start_time": "2026-03-02T06:29:40.720788698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#We count the number of traininable parameters in the functuion which is then used for model comparision\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ],
   "id": "3e94bd03d5a67e26",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-02T06:29:40.798463480Z",
     "start_time": "2026-03-02T06:29:40.772011975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#This block is all of plain CNN architecture\n",
    "class PlainCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        #We then finally map all the 10 logits\n",
    "        self.classifier = nn.Linear(256, num_classes)\n",
    "    #\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n",
    "#properties of the CNN layer wehre deep features are extracted, channels are increased. We downsample to reudce the spatial size and final global avg pooling reduces parameters"
   ],
   "id": "505ea2ac1a4cf10f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-02T06:29:40.858419474Z",
     "start_time": "2026-03-02T06:29:40.800829186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "        return out"
   ],
   "id": "75b83e8f12c76fae",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-02T06:29:40.920787395Z",
     "start_time": "2026-03-02T06:29:40.860858080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResidualCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.layer1 = ResidualBlock(64, 64)\n",
    "        self.layer2 = ResidualBlock(64, 128, stride=2)\n",
    "        self.layer3 = ResidualBlock(128, 256, stride=2)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)"
   ],
   "id": "a80ecaf1c2cf81e5",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#This is an evaluation function we use to set the model to evaluation mode to disable gradiants, compute loss and accuracy\n",
    "#To remmeber:\n",
    "#1. We are using .eval() to disable dropout to change the BN behaviour\n",
    "#2. no_grad- like we used in the week 3 we use this to save  a lot of memory\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    loss_total = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs,y)\n",
    "            loss_total += loss.item()\n",
    "\n",
    "            _,pred = outputs.max(1)\n",
    "            correct += pred.eq(y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return loss_total/len(loader), 100*correct/total\n",
    "\n",
    "#This is a training model responsible for the fllowing:\n",
    "# forward ppass --> compute loss --> backward pass --> optimizer step --> LR Scheduler step\n",
    "def train_model(model, train_loader, val_loader, epochs=30, lr=1e-3):\n",
    "    model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=lr,\n",
    "        epochs=epochs,\n",
    "        steps_per_epoch=len(train_loader)\n",
    "    )\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    history = {\"train_loss\":[], \"val_loss\":[],\"train_acc\":[], \"val_acc\":[]}\n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for x,y in train_loader:\n",
    "            x,y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs,y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            train_loss += loss.item()\n",
    "            _,pred = outputs.max(1)\n",
    "            correct += pred.eq(y).sum().item()\n",
    "            total += y.size(0)\n",
    "        train_acc = 100*correct/total\n",
    "        val_loss, val_acc = evaluate(model, val_loader)\n",
    "        history[\"train_loss\"].append(train_loss/len(train_loader))\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        print(f\"Epoch {epoch+1}: \"f\"Train Acc {train_acc:.2f} | \"f\"Val Acc {val_acc:.2f}\")\n",
    "    training_time = (time.time()-start)/60\n",
    "    return history, training_time\n",
    "#History tracking: stores all the losses and accuracies (train and validation) ---> this is used for learning curves and overfitting gaps"
   ],
   "id": "80bbc1e86f1c7cdb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-02T06:29:41.112586905Z",
     "start_time": "2026-03-02T06:29:41.017213093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def plot_learning_curves(history, label):\n",
    "    plt.plot(history[\"train_acc\"], label=f\"{label} Train\")\n",
    "    plt.plot(history[\"val_acc\"], label=f\"{label} Val\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()"
   ],
   "id": "28a56cf9c29c27ca",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-02T06:29:41.208453980Z",
     "start_time": "2026-03-02T06:29:41.167747695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_metrics(model, loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(device)\n",
    "            outputs = model(x)\n",
    "            _,pred = outputs.max(1)\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(y.numpy())\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=None)\n",
    "    macro = precision_recall_fscore_support(all_labels, all_preds, average=\"macro\")\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    return precision, recall, f1, macro, cm"
   ],
   "id": "72e79fe7c1713143",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-02T06:29:41.287166348Z",
     "start_time": "2026-03-02T06:29:41.228703111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#WE now compute the metrics like accuracy precision and recall for the ocnfusion matrix\n",
    "def compute_metrics(model, loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(device)\n",
    "            outputs = model(x)\n",
    "            _,pred = outputs.max(1)\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(y.numpy())\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average=None\n",
    "    )\n",
    "    macro = precision_recall_fscore_support(all_labels, all_preds, average=\"macro\")\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    return precision, recall, f1, macro, cm"
   ],
   "id": "f75ce172f9eda402",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-02T06:29:41.347114525Z",
     "start_time": "2026-03-02T06:29:41.290116575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#This function for misclassificaiton is being shown to find the confidence of the model aand find out the wrong predictiosn\n",
    "#THis makes sure taht we can do a proper error analysis and find out failures systematically\n",
    "def show_misclassified(model, loader, num=12):\n",
    "    model.eval()\n",
    "    images = []\n",
    "    preds = []\n",
    "    labels = []\n",
    "    probs = []\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(device)\n",
    "            outputs = model(x)\n",
    "            p = F.softmax(outputs, dim=1)\n",
    "            conf, pred = p.max(1)\n",
    "            for i in range(len(y)):\n",
    "                if pred[i] != y[i]:\n",
    "                    images.append(x[i].cpu())\n",
    "                    preds.append(pred[i].item())\n",
    "                    labels.append(y[i].item())\n",
    "                    probs.append(conf[i].item())\n",
    "                    if len(images) >= num:\n",
    "                        break\n",
    "            if len(images) >= num:\n",
    "                break\n",
    "    fig, axes = plt.subplots(3,4, figsize=(12,8))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        img = images[i]*0.5 + 0.5\n",
    "        ax.imshow(img.permute(1,2,0))\n",
    "        ax.set_title(f\"P:{preds[i]} ({probs[i]:.2f})\\nT:{labels[i]}\")\n",
    "        ax.axis(\"off\")\n",
    "    plt.show()"
   ],
   "id": "f9fa2c2f48ba9068",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-02T06:29:41.437716904Z",
     "start_time": "2026-03-02T06:29:41.349867384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "#We are now going to be computing a master table for the strucutred comparision of MLP, Plain CNN, Residual CNN\n",
    "def build_master_table(mlp_stats, plain_stats, res_stats):\n",
    "    df = pd.DataFrame({\n",
    "        \"Model\": [\"Week 3 MLP\", \"Plain CNN\", \"Residual CNN\"],\n",
    "        \"Params (k)\": [\n",
    "            mlp_stats[\"params\"]/1000,\n",
    "            plain_stats[\"params\"]/1000,\n",
    "            res_stats[\"params\"]/1000\n",
    "        ],\n",
    "        \"Test Acc (%)\": [\n",
    "            mlp_stats[\"test_acc\"],\n",
    "            plain_stats[\"test_acc\"],\n",
    "            res_stats[\"test_acc\"]\n",
    "        ],\n",
    "        \"Overfitting Gap (%)\": [\n",
    "            mlp_stats[\"gap\"],\n",
    "            plain_stats[\"gap\"],\n",
    "            res_stats[\"gap\"]\n",
    "        ],\n",
    "        \"Training Time (min)\": [\n",
    "            mlp_stats[\"time\"],\n",
    "            plain_stats[\"time\"],\n",
    "            res_stats[\"time\"]\n",
    "        ]\n",
    "    })\n",
    "    return df"
   ],
   "id": "e8a1614fe46ffa30",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-02T06:29:43.769632464Z",
     "start_time": "2026-03-02T06:29:41.464038478Z"
    }
   },
   "cell_type": "code",
   "source": "train_loader, val_loader, test_loader = get_dataloaders(batch_size=128)",
   "id": "aeab942bc7f09d72",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sridhar-devarakonda/PyCharmMiscProject/.venv/lib/python3.12/site-packages/torchvision/datasets/cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  entry = pickle.load(f, encoding=\"latin1\")\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Now we traint he plain CNN: where test accuracy, parameter count and overfitting gap is evaluated\n",
    "plain_model = PlainCNN()\n",
    "plain_history, plain_time = train_model(\n",
    "    plain_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs=30,\n",
    "    lr=1e-3\n",
    ")\n",
    "plain_test_loss, plain_test_acc = evaluate(plain_model,test_loader)\n",
    "plain_params = count_parameters(plain_model)\n",
    "plain_gap = plain_history[\"train_acc\"][-1] - plain_history[\"val_acc\"][-1]"
   ],
   "id": "5fc97c35317fa85f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-03-02T07:23:21.101404086Z",
     "start_time": "2026-03-02T07:12:26.539853424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Here we traint he residual CNN network for the same aspects as the one for the plain CNN\n",
    "res_model = ResidualCNN()\n",
    "res_history, res_time = train_model(\n",
    "    res_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs=30,\n",
    "    lr=1e-3\n",
    ")\n",
    "res_test_loss, res_test_acc = evaluate(res_model,test_loader)\n",
    "res_params = count_parameters(res_model)\n",
    "res_gap = res_history[\"train_acc\"][-1] - res_history[\"val_acc\"][-1]"
   ],
   "id": "76ccfee1f44b5a40",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m res_model = ResidualCNN()\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m res_history, res_time = \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m    \u001B[49m\u001B[43mres_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m30\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1e-3\u001B[39;49m\n\u001B[32m      8\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m      9\u001B[39m res_test_loss, res_test_acc = evaluate(res_model,test_loader)\n\u001B[32m     10\u001B[39m res_params = count_parameters(res_model)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 37\u001B[39m, in \u001B[36mtrain_model\u001B[39m\u001B[34m(model, train_loader, val_loader, epochs, lr)\u001B[39m\n\u001B[32m     35\u001B[39m correct = \u001B[32m0\u001B[39m\n\u001B[32m     36\u001B[39m total = \u001B[32m0\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m37\u001B[39m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43my\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     38\u001B[39m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43my\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m.\u001B[49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     39\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mzero_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmMiscProject/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:741\u001B[39m, in \u001B[36m_BaseDataLoaderIter.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    738\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    739\u001B[39m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[32m    740\u001B[39m     \u001B[38;5;28mself\u001B[39m._reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m741\u001B[39m data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    742\u001B[39m \u001B[38;5;28mself\u001B[39m._num_yielded += \u001B[32m1\u001B[39m\n\u001B[32m    743\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    744\u001B[39m     \u001B[38;5;28mself\u001B[39m._dataset_kind == _DatasetKind.Iterable\n\u001B[32m    745\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    746\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m._num_yielded > \u001B[38;5;28mself\u001B[39m._IterableDataset_len_called\n\u001B[32m    747\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmMiscProject/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:801\u001B[39m, in \u001B[36m_SingleProcessDataLoaderIter._next_data\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    799\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    800\u001B[39m     index = \u001B[38;5;28mself\u001B[39m._next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m801\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[32m    802\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._pin_memory:\n\u001B[32m    803\u001B[39m         data = _utils.pin_memory.pin_memory(data, \u001B[38;5;28mself\u001B[39m._pin_memory_device)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmMiscProject/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[39m, in \u001B[36m_MapDatasetFetcher.fetch\u001B[39m\u001B[34m(self, possibly_batched_index)\u001B[39m\n\u001B[32m     50\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.auto_collation:\n\u001B[32m     51\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.dataset, \u001B[33m\"\u001B[39m\u001B[33m__getitems__\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.dataset.__getitems__:\n\u001B[32m---> \u001B[39m\u001B[32m52\u001B[39m         data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m.\u001B[49m\u001B[43m__getitems__\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     53\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     54\u001B[39m         data = [\u001B[38;5;28mself\u001B[39m.dataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmMiscProject/.venv/lib/python3.12/site-packages/torch/utils/data/dataset.py:413\u001B[39m, in \u001B[36mSubset.__getitems__\u001B[39m\u001B[34m(self, indices)\u001B[39m\n\u001B[32m    411\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.dataset.__getitems__([\u001B[38;5;28mself\u001B[39m.indices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[32m    412\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m413\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmMiscProject/.venv/lib/python3.12/site-packages/torchvision/datasets/cifar.py:119\u001B[39m, in \u001B[36mCIFAR10.__getitem__\u001B[39m\u001B[34m(self, index)\u001B[39m\n\u001B[32m    116\u001B[39m img = Image.fromarray(img)\n\u001B[32m    118\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m119\u001B[39m     img = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    121\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.target_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    122\u001B[39m     target = \u001B[38;5;28mself\u001B[39m.target_transform(target)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmMiscProject/.venv/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001B[39m, in \u001B[36mCompose.__call__\u001B[39m\u001B[34m(self, img)\u001B[39m\n\u001B[32m     93\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[32m     94\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transforms:\n\u001B[32m---> \u001B[39m\u001B[32m95\u001B[39m         img = \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     96\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmMiscProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1776\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1774\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1775\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1776\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmMiscProject/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1787\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1784\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1786\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1787\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1789\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1790\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmMiscProject/.venv/lib/python3.12/site-packages/torchvision/transforms/transforms.py:721\u001B[39m, in \u001B[36mRandomHorizontalFlip.forward\u001B[39m\u001B[34m(self, img)\u001B[39m\n\u001B[32m    713\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    714\u001B[39m \u001B[33;03mArgs:\u001B[39;00m\n\u001B[32m    715\u001B[39m \u001B[33;03m    img (PIL Image or Tensor): Image to be flipped.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    718\u001B[39m \u001B[33;03m    PIL Image or Tensor: Randomly flipped image.\u001B[39;00m\n\u001B[32m    719\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    720\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch.rand(\u001B[32m1\u001B[39m) < \u001B[38;5;28mself\u001B[39m.p:\n\u001B[32m--> \u001B[39m\u001B[32m721\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhflip\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    722\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PyCharmMiscProject/.venv/lib/python3.12/site-packages/torchvision/transforms/functional.py:654\u001B[39m, in \u001B[36mhflip\u001B[39m\u001B[34m(img)\u001B[39m\n\u001B[32m    650\u001B[39m     img = resize(img, size, interpolation, antialias=antialias)\n\u001B[32m    651\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n\u001B[32m--> \u001B[39m\u001B[32m654\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mhflip\u001B[39m(img: Tensor) -> Tensor:\n\u001B[32m    655\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Horizontally flip the given image.\u001B[39;00m\n\u001B[32m    656\u001B[39m \n\u001B[32m    657\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    664\u001B[39m \u001B[33;03m        PIL Image or Tensor:  Horizontally flipped image.\u001B[39;00m\n\u001B[32m    665\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m    666\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch.jit.is_scripting() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch.jit.is_tracing():\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plot_learning_curves(plain_history, \"Plain CNN\")\n",
    "plot_learning_curves(res_history, \"Residual CNN\")\n",
    "plt.title(\"Learning Curves\")\n",
    "plt.show()"
   ],
   "id": "bed54e9e81f77770",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "precision, recall, f1, macro, cm = compute_metrics(res_model, test_loader)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()"
   ],
   "id": "3101e851258473e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "show_misclassified(res_model, test_loader)",
   "id": "1665c679f980b24e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#So to summarize our complete pipeline here:\n",
   "id": "bec712f6638b2b8c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
